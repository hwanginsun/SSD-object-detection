{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils import DetectionDataset, draw_rectangle\n",
    "from models.generator import DetectionGenerator, PriorBoxes\n",
    "from models.ssd import build_base_network, attach_multibox_head\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ 모델 학습 \\]\n",
    "----\n",
    "----\n",
    "\n",
    "우리는 학습할 모델과 데이터를 이전 시간에서 꾸렸습니다. \n",
    "\n",
    "> 이제 학습까지 남은 것은 `LOSS` 함수를 어떻게 설계할까? 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loss 계산하기\n",
    "----\n",
    "\n",
    "원문에 있는 설명을 가져왔습니다. 우리가 학습해야 하는 것은 위치를 추론하는 Regressor와 사물을 분류하는 Classifer입니다. Regressor의 경우에는 SmoothL1이라 불리는 Loss로 학습을 시키고, Classifier은 분류모델에서 주로 사용하는 Cross-Entropy Loss를 이용합니다.<br>\n",
    "Regressor의 경우에는 당연하게도 Matched prior box의 경우 한에서만 학습을 해야합니다.\n",
    "\n",
    "The SSD Training Objective is derived from the MultiBox Objective, but is extended to handle multiple object categories. Let $x^p_{ij}= {1,0}$ be an indicator for matching the i-th default box to the j-th ground truth box of category $p$. In the matching strategy above, we can have $\\sum_i x^{p}_{ij} \\ge 1$. The overall objective loss function is a weighted sum of the localization loss(loc) and the confidence loss(conf):\n",
    "\n",
    "$\n",
    "L(x,c,l,g)= \\frac{1}{N}(L_{conf}(x,c)+\\alpha L_{loc}(x,l,g)) \n",
    "$\n",
    "\n",
    "Where N is the number of matched default boxes. if $N = 0$, we set the loss to 0. The localization loss is a Smooth L1 loss between the predicted box(l) and the ground truth box(g) parameters. Similar to Faster R-CNN, we regress to offsets for the center$(cx,cy)$ of the default bounding box(p) and for its width(w) and height (h).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "L_{loc}(x,l,g) = \\sum^{N}_{i\\in Pos} \\sum_{m \\in {cx,cy,w,h}} x_{ij}^k smooth_{L1}(l^m_i-\\hat g^m_j) \\\\\n",
    "\\hat g^{cx} = \\frac{(g^{cx} - p^{cx})}{p^{w}}, \\hat g^{cy} = \\frac{(g^{cy} - p^{cy})}{p^{h}}\\\\  \n",
    "\\hat g^{w} = log(\\frac{g^{w}}{p^{w}}), \\hat g^{h} = log(\\frac{g^{h}}{p^{h}}) \\\\\n",
    "$<br>\n",
    "The confidence loss is the softmax loss over multiple classes confidences (c).<br>\n",
    "$\n",
    "L_{conf}(x,c) = - \\sum^{N}_{i\\in Pos} x^{P}_{ij}log(\\hat c^p_i) - \\sum^{N}_{i\\in Neg} log(\\hat c_i^p) where \\hat c^p_i = \\frac{exp(c_i^p)}{\\sum_p exp(c^p_i)}\n",
    "$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confidence Loss를 계산할 때, 중요한 문제가 하나 있습니다. 바로 Class Imbalance 문제입니다. 영상에서 대부분은 BackGround에 해당합니다. 우리가 원하는 Foreground에 매칭된 Prior box는 극히 일부분에 불과합니다. 이 때문에, Easy Negative Sample, 즉 Background에 대한 Loss가 지나치게 커서, 실제로 학습하고자 하는 Foreground에 대한 학습은 잘 이루어지지 않게 됩니다. 이를 방지하기 위해, Negative Sample 중 가장 Loss가 큰것들을 위주로만 추출하여 학습하도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSDLoss(alpha=1., pos_neg_ratio=3.):\n",
    "    def ssd_loss(y_true, y_pred):\n",
    "        num_classes = tf.shape(y_true)[2] - 4\n",
    "        y_true = tf.reshape(y_true, [-1, num_classes + 4])\n",
    "        y_pred = tf.reshape(y_pred, [-1, num_classes + 4])\n",
    "        eps = K.epsilon()\n",
    "\n",
    "        # Split Classification and Localization output\n",
    "        y_true_clf, y_true_loc = tf.split(y_true, \n",
    "                                          [num_classes, 4], \n",
    "                                          axis=-1)\n",
    "        y_pred_clf, y_pred_loc = tf.split(y_pred, \n",
    "                                          [num_classes, 4], \n",
    "                                          axis=-1)\n",
    "\n",
    "        # split foreground & background\n",
    "        neg_mask = y_true_clf[:, -1]\n",
    "        pos_mask = 1 - neg_mask\n",
    "        num_pos = tf.reduce_sum(pos_mask)\n",
    "        num_neg = tf.reduce_sum(neg_mask)\n",
    "        num_neg = tf.minimum(pos_neg_ratio * num_pos, num_neg)\n",
    "\n",
    "        # softmax loss\n",
    "        y_pred_clf = K.clip(y_pred_clf, eps, 1. - eps)\n",
    "        clf_loss = -tf.reduce_sum(y_true_clf * tf.log(y_pred_clf),\n",
    "                                  axis=-1)\n",
    "        pos_clf_loss = tf.reduce_sum(clf_loss * pos_mask) / (num_pos + eps)\n",
    "        neg_clf_loss = clf_loss * neg_mask\n",
    "        values, indices = tf.nn.top_k(neg_clf_loss,\n",
    "                                      k=tf.cast(num_neg, tf.int32))\n",
    "        neg_clf_loss = tf.reduce_sum(values) / (num_neg + eps)\n",
    "        clf_loss = pos_clf_loss + neg_clf_loss\n",
    "        \n",
    "        # smooth l1 loss\n",
    "        l1_loss = tf.abs(y_true_loc - y_pred_loc)\n",
    "        l2_loss = 0.5 * (y_true_loc - y_pred_loc) ** 2\n",
    "        loc_loss = tf.where(tf.less(l1_loss, 1.0),\n",
    "                            l2_loss,\n",
    "                            l1_loss - 0.5)\n",
    "        loc_loss = tf.reduce_sum(loc_loss, axis=-1)\n",
    "        loc_loss = tf.reduce_sum(loc_loss * pos_mask) / (num_pos + eps)\n",
    "\n",
    "        # total loss\n",
    "        return clf_loss + alpha * loc_loss\n",
    "    return ssd_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 구성하기\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_network = build_base_network(num_units=16)\n",
    "predictions = attach_multibox_head(base_network,\n",
    "                                   ['norm3_2','norm4_2','norm5_2'])\n",
    "model = Model(base_network.input, predictions,name='ssd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 구성하기\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = DetectionDataset(data_type='train')\n",
    "\n",
    "prior = PriorBoxes(strides=[4, 8, 16],\n",
    "           scales=[10, 25, 40],\n",
    "           ratios=[0.4,0.8, 1.2])\n",
    "\n",
    "traingen = DetectionGenerator(trainset, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam',loss=ssd_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/343 [====================>.........] - ETA: 3:50 - loss: 7.4274"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-81ac11980b9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraingen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(traingen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
