{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils import DetectionDataset, draw_rectangle\n",
    "from models.generator import DetectionGenerator, PriorBoxes\n",
    "from models.ssd import build_base_network, attach_multibox_head\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(10)\n",
    "tf.random.set_random_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ 모델 학습 \\]\n",
    "----\n",
    "----\n",
    "\n",
    "우리는 학습할 모델과 데이터를 이전 시간에서 꾸렸습니다. 이제 학습까지 남은 것은 모델을 어떻게 학습시킬까?입니다. 이 중 제일 핵심은 바로 Loss함수 설계에 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loss 계산하기\n",
    "----\n",
    "\n",
    "원문에 있는 설명을 가져왔습니다. 우리가 학습해야 하는 것은 위치를 추론하는 Regressor와 사물을 분류하는 Classifer입니다. Regressor의 경우에는 SmoothL1이라 불리는 Loss로 학습을 시키고, Classifier은 분류모델에서 주로 사용하는 Cross-Entropy Loss를 이용합니다. Regressor의 경우에는 당연하게도 Matched prior box의 경우 한에서만 학습을 해야 합니다. SmoothL1은 MAE와 MSE의 합쳐놓은 형태로, 수식은 아래와 같습니다.\n",
    "\n",
    "$\n",
    "smooth_{L1}(x) = \\begin{cases}\n",
    "0.5x^2, \\mbox{  if  } |x| <1\\\\\n",
    "|x| - 0.5 \\mbox{   otherwise,}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "The SSD Training Objective is derived from the MultiBox Objective, but is extended to handle multiple object categories. Let $x^p_{ij}= {1,0}$ be an indicator for matching the i-th default box to the j-th ground truth box of category $p$. In the matching strategy above, we can have $\\sum_i x^{p}_{ij} \\ge 1$. The overall objective loss function is a weighted sum of the localization loss(loc) and the confidence loss(conf):\n",
    "\n",
    "$\n",
    "L(x,c,l,g)= \\frac{1}{N}(L_{conf}(x,c)+\\alpha L_{loc}(x,l,g)) \n",
    "$\n",
    "\n",
    "Where N is the number of matched default boxes. if $N = 0$, we set the loss to 0. The localization loss is a Smooth L1 \n",
    "loss between the predicted box(l) and the ground truth box(g) parameters. Similar to Faster R-CNN, we regress to offsets for the center$(cx,cy)$ of the default bounding box(p) and for its width(w) and height (h).<br>\n",
    "\n",
    "$\n",
    "L_{loc}(x,l,g) = \\sum^{N}_{i\\in Pos} \\sum_{m \\in {cx,cy,w,h}} x_{ij}^k smooth_{L1}(l^m_i-\\hat g^m_j) \\\\\n",
    "\\hat g^{cx} = \\frac{(g^{cx} - p^{cx})}{p^{w}}, \\hat g^{cy} = \\frac{(g^{cy} - p^{cy})}{p^{h}}\\\\  \n",
    "\\hat g^{w} = log(\\frac{g^{w}}{p^{w}}), \\hat g^{h} = log(\\frac{g^{h}}{p^{h}}) \\\\\n",
    "$<br>\n",
    "The confidence loss is the softmax loss over multiple classes confidences (c).<br>\n",
    "$\n",
    "L_{conf}(x,c) = - \\sum^{N}_{i\\in Pos} x^{P}_{ij}log(\\hat c^p_i) - \\sum^{N}_{i\\in Neg} log(\\hat c_i^p) where \\hat c^p_i = \\frac{exp(c_i^p)}{\\sum_p exp(c^p_i)}\n",
    "$<br>\n",
    "\n",
    "Confidence Loss를 계산할 때, 중요한 문제가 하나 있습니다. 바로 Class Imbalance 문제입니다. 영상에서 대부분은 BackGround에 해당합니다. 우리가 원하는 Foreground에 매칭된 Prior box는 극히 일부분에 불과합니다. 이 때문에, Easy Negative Sample, 즉 Background에 대한 Loss가 지나치게 커서, 실제로 학습하고자 하는 Foreground에 대한 학습은 잘 이루어지지 않게 됩니다. 이를 방지하기 위해, Negative Sample 중 가장 Loss가 큰것들을 위주로만 추출하여 학습하도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSDLoss(alpha=1., pos_neg_ratio=3.):\n",
    "    def ssd_loss(y_true, y_pred):\n",
    "        num_classes = tf.shape(y_true)[2] - 4\n",
    "        y_true = tf.reshape(y_true, [-1, num_classes + 4])\n",
    "        y_pred = tf.reshape(y_pred, [-1, num_classes + 4])\n",
    "        eps = K.epsilon()\n",
    "\n",
    "        # Split Classification and Localization output\n",
    "        y_true_clf, y_true_loc = tf.split(y_true, \n",
    "                                          [num_classes, 4], \n",
    "                                          axis=-1)\n",
    "        y_pred_clf, y_pred_loc = tf.split(y_pred, \n",
    "                                          [num_classes, 4], \n",
    "                                          axis=-1)\n",
    "\n",
    "        # split foreground & background\n",
    "        neg_mask = y_true_clf[:, -1]\n",
    "        pos_mask = 1 - neg_mask\n",
    "        num_pos = tf.reduce_sum(pos_mask)\n",
    "        num_neg = tf.reduce_sum(neg_mask)\n",
    "        num_neg = tf.minimum(pos_neg_ratio * num_pos, num_neg)\n",
    "\n",
    "        # softmax loss\n",
    "        y_pred_clf = K.clip(y_pred_clf, eps, 1. - eps)\n",
    "        clf_loss = -tf.reduce_sum(y_true_clf * tf.log(y_pred_clf),\n",
    "                                  axis=-1)\n",
    "        pos_clf_loss = tf.reduce_sum(clf_loss * pos_mask) / (num_pos + eps)\n",
    "        neg_clf_loss = clf_loss * neg_mask\n",
    "        values, indices = tf.nn.top_k(neg_clf_loss,\n",
    "                                      k=tf.cast(num_neg, tf.int32))\n",
    "        neg_clf_loss = tf.reduce_sum(values) / (num_neg + eps)\n",
    "        clf_loss = pos_clf_loss + neg_clf_loss\n",
    "        \n",
    "        # smooth l1 loss\n",
    "        l1_loss = tf.abs(y_true_loc - y_pred_loc)\n",
    "        l2_loss = 0.5 * (y_true_loc - y_pred_loc) ** 2\n",
    "        loc_loss = tf.where(tf.less(l1_loss, 1.0),\n",
    "                            l2_loss,\n",
    "                            l1_loss - 0.5)\n",
    "        loc_loss = tf.reduce_sum(loc_loss, axis=-1)\n",
    "        loc_loss = tf.reduce_sum(loc_loss * pos_mask) / (num_pos + eps)\n",
    "\n",
    "        # total loss\n",
    "        return clf_loss + alpha * loc_loss\n",
    "    return ssd_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 구성하기\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "strides = [4, 8, 16]\n",
    "scales = [10, 25, 40]\n",
    "ratios = [(1,1),     # ratio : 1.\n",
    "          (0.5,1.5), # ratio : 0.33\n",
    "          (0.8,1.2), # ratio : 0.67\n",
    "          (1.2,0.8), # ratio : 1.5\n",
    "          (1.4,1.4)]\n",
    "\n",
    "prior = PriorBoxes(strides,scales,ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0730 09:12:15.532675 4598965696 deprecation.py:506] From /Users/ksj/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "base_network = build_base_network(num_units=16)\n",
    "predictions = attach_multibox_head(base_network,\n",
    "                                   ['norm3_2','norm4_2','norm5_2'],\n",
    "                                   num_priors=len(ratios))\n",
    "model = Model(base_network.input,\n",
    "              predictions,\n",
    "              name='ssd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 구성하기\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = DetectionDataset(data_type='train')\n",
    "validset = DetectionDataset(data_type='validation')\n",
    "\n",
    "# .config로 했을 때에만 use_multiprocessing이 가능함\n",
    "# Argument로 들어가는 것들이 Serialize되는 Class이어야 되기 때문\n",
    "traingen = DetectionGenerator(trainset.config, \n",
    "                              prior.config, \n",
    "                              batch_size=64)\n",
    "validgen = DetectionGenerator(validset.config, \n",
    "                              prior.config, \n",
    "                              batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 컴파일 및 학습하기\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.losses import SSDLoss\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0730 09:12:45.060121 4598965696 deprecation_wrapper.py:119] From ../models/losses.py:48: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0730 09:12:45.082269 4598965696 deprecation.py:323] From ../models/losses.py:62: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(Adam(1e-3),\n",
    "              loss=SSDLoss(1.0,3.))\n",
    "\n",
    "callbacks =[]\n",
    "rlrop = ReduceLROnPlateau(factor=0.1, \n",
    "                          min_lr=1e-6, \n",
    "                          patience=5,\n",
    "                          cooldown=3)\n",
    "callbacks.append(rlrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  2/171 [..............................] - ETA: 34:46 - loss: 13.3759"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Keras_worker_ForkPoolWorker-4:\n",
      "Process Keras_worker_ForkPoolWorker-6:\n",
      "Process Keras_worker_ForkPoolWorker-2:\n",
      "Process Keras_worker_ForkPoolWorker-1:\n",
      "Process Keras_worker_ForkPoolWorker-3:\n",
      "Process Keras_worker_ForkPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 347, in put\n",
      "    self._writer.send_bytes(obj)\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 398, in _send_bytes\n",
      "    self._send(buf)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c86723c0a7af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     callbacks=callbacks)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(traingen,\n",
    "                    epochs=20,\n",
    "                    validation_data=validgen,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=6,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델을 통해 예측하기\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_maximum_suppression(boxes, confs, overlap_threshold=0.5):\n",
    "    pick = []\n",
    "    cx, cy, w, h = boxes.T\n",
    "    x1, x2 = cx - w/2, cx + w/2\n",
    "    y1, y2 = cy - h/2, cy + h/2    \n",
    "    \n",
    "    indices = np.argsort(confs)[::-1]\n",
    "    area = (x2 - x1) * (y2 - y1)\n",
    "    \n",
    "    while len(indices) > 1:\n",
    "        idx, indices = indices[0], indices[1:]\n",
    "        pick.append(idx)\n",
    "\n",
    "        xx1 = np.maximum(x1[idx], x1[indices])\n",
    "        yy1 = np.maximum(y1[idx], y1[indices])\n",
    "        xx2 = np.minimum(x2[idx], x2[indices])\n",
    "        yy2 = np.minimum(y2[idx], y2[indices])\n",
    "        \n",
    "        w = np.maximum(0, xx2 - xx1)\n",
    "        h = np.maximum(0, yy2 - yy1)\n",
    "        \n",
    "        overlap = w * h  / (area[indices] + 1e-8)\n",
    "        indices = indices[overlap <= overlap_threshold]\n",
    "        \n",
    "    return pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = validgen[0]\n",
    "predictions = model.predict(images)\n",
    "\n",
    "for idx, image in enumerate(images):\n",
    "    pred_loc = predictions[idx,:,-4:]\n",
    "    pred_clf = predictions[idx,:,:-4]\n",
    "    pr_boxes = prior.generate(images[0].shape)\n",
    "\n",
    "    # 복원 후 동일한지 확인하기\n",
    "    res_cx = (pred_loc[:,0] \n",
    "              * pr_boxes[:,2] \n",
    "              + pr_boxes[:,0])\n",
    "    res_cy = (pred_loc[:,1] \n",
    "              * pr_boxes[:,3] \n",
    "              + pr_boxes[:,1])\n",
    "    res_w = (np.exp(pred_loc[:,2])\n",
    "             *pr_boxes[:,2])\n",
    "    res_h = (np.exp(pred_loc[:,3])\n",
    "             *pr_boxes[:,3])\n",
    "\n",
    "    restore_boxes = np.stack([res_cx,res_cy,res_w,res_h],\n",
    "                             axis=-1)\n",
    "    fg_indices = np.argwhere(pred_clf.argmax(axis=1)!=10)\n",
    "    restore_boxes = restore_boxes[fg_indices].squeeze()\n",
    "    pred_clf = pred_clf[fg_indices]\n",
    "    pred_clf = pred_clf.max(axis=-1)\n",
    "    pred_clf = pred_clf.squeeze()\n",
    "    \n",
    "    vis = draw_rectangle(images[idx],\n",
    "                  pd.DataFrame(restore_boxes, \n",
    "                               columns=['cx','cy','w','h'])\n",
    "                  )\n",
    "    plt.imshow(vis)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])    \n",
    "    plt.show()        \n",
    "    \n",
    "    pick_indices = non_maximum_suppression(restore_boxes, pred_clf, 0.5)\n",
    "    chosen = restore_boxes[pick_indices]\n",
    "    \n",
    "    vis = draw_rectangle(images[idx],\n",
    "                  pd.DataFrame(chosen, \n",
    "                               columns=['cx','cy','w','h'])\n",
    "                  )\n",
    "    plt.imshow(vis)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])    \n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
